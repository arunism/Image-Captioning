{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_input() is used to preprocess any given image to extract features of that image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "# load_img() is used to load image from file as a pil image\n",
    "# img_to_array() is used to convert pil image instance to a numpy array so that our model can understand/interpret the image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "# Model() can be instanciated to include the necessary layers given some input arrays or tensors to output arrays or tensors\n",
    "from tensorflow.keras.models import Model\n",
    "# Pickle is used to serialize and deserialize the python object structure so that any object on python can be pickled and saved to the disk\n",
    "# So pickle.dumb() fuction is used to save object data to the file\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Image Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes document text as arguments\n",
    "# And returns dictonary of image identifiers and corresponding descriptions\n",
    "def read_image_descriptions(filename):\n",
    "    # Open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # Read all the text from the file\n",
    "    text = file.read()\n",
    "    # After reading the file completely close the file\n",
    "    file.close()\n",
    "    \n",
    "    # Initializing a dictionary named image_id_dict\n",
    "    image_id_dict = dict()\n",
    "    \n",
    "    # Incorporating for loop to read each sentence from the text by splitting them on the basis of new line character ('\\n')\n",
    "    for line in text.split('\\n'):\n",
    "        # Splitting each sentence obtained from above on the basis of white space into indivudial words/tokens\n",
    "        # You can also use nltk classes like words_tokenize() to tokenize sentence to words/tokens\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        # Now for each sentence we treat first token as image id and rest of the tokens as image description\n",
    "        image_id, image_description = tokens[0], tokens[1:]\n",
    "        # Remove file extension from the image_id\n",
    "        image_id = image_id.split('.')[0]\n",
    "        # Convert image description tokens back to string\n",
    "        image_description = ' '.join(image_description)\n",
    "        # Create the list if needed\n",
    "        if image_id not in image_id_dict:\n",
    "            image_id_dict[image_id] = list()\n",
    "        # Store description\n",
    "        image_id_dict[image_id].append(image_description)\n",
    "        \n",
    "    return image_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion to clean description of image\n",
    "# This function takes dictionary of image_id and description as input\n",
    "def clean_description_text(description):\n",
    "    # Prepare translation table for removing punctuations\n",
    "    # This uses the 3-argument version of str.maketrans with arguments (x, y, z) where 'x' and 'y' must be equal-length strings\n",
    "    # Characters in 'x' are replaced by characters in 'y'.\n",
    "    # 'z' is a string (string.punctuation here) where each character in the string is mapped to None\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    for key, desc_list in description.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            # Get the description at i'th index from all the given five descriptions for that image\n",
    "            desc = desc_list[i]\n",
    "            # Tokenize(grab each words as list) the description by splitting them based on whitespaces\n",
    "            desc = desc.split()\n",
    "            # Convert the words to lower case\n",
    "            desc = [word.lower() for word in desc]\n",
    "            # Remove punctions from each token\n",
    "            # translate() method returns a string where each character is mapped to its corresponding character in the translation table\n",
    "            desc = [word.translate(translator) for word in desc]\n",
    "            # Removing single charactered words\n",
    "            # Removing hanging 's' and 'a'\n",
    "            desc = [word for word in desc if len(word)>1]\n",
    "            # Take only alpahabets containing words and exclude those containing numbers or other special characters\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            # Convert the i'th image description tokens back to string\n",
    "            desc_list[i] = ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save description to file\n",
    "# description argument is a dictionary which contains mapping of image identifiers to the corresponding descriptions\n",
    "# filename argument is the name, we want to give to the file in which we want to save the mapping of image identifiers to cleaned descriptions\n",
    "def save_description(description, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in description.items():\n",
    "        for desc in desc_list:\n",
    "            # Save description of the image preceded by the image identifier\n",
    "            lines.append(key + \" \" + desc)\n",
    "    # Convert all the descriptions list as string into new lines\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of descriptions: 8092\n"
     ]
    }
   ],
   "source": [
    "# Calling the above created functions in a series\n",
    "filename = './Datasets/Flickr8k.token.txt'\n",
    "descriptions = read_image_descriptions(filename)\n",
    "print(f\"Total number of descriptions: {len(descriptions)}\")\n",
    "clean_description_text(descriptions)\n",
    "save_description(descriptions, 'VGG16_descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuction to extract features from each image to the directory\n",
    "def get_image_features(directory):\n",
    "    # Create an instance of the VGG16 model\n",
    "    model = VGG16()\n",
    "    # Restructuring our VGG16 model by removing/Popping off the last layer of the model\n",
    "    # The last layer is used to classify the images. Since we are not classifying images here, we're removing the last layer\n",
    "    model.layers.pop()\n",
    "    # Keras model represents the actual neural network model.\n",
    "    # Keras provides a two mode to create the model, simple and easy to use Sequential API as well as more flexible and advanced Functional API.\n",
    "    # A ANN model can be created by simply calling Sequential() API\n",
    "    # Sequential API is used to create models layer-by-layer\n",
    "    # Functional model, you can define multiple input or output that share layers.\n",
    "    # First, we create an instance for model using Model class and connect to the layers to access input and output to the model\n",
    "    # model.inputs is the input fed to the model and model.layers[-1].output is the output of the last(-1) layer of the model\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    # Print the summary of the model\n",
    "    print(model.summary())\n",
    "    # This empty dictionary will be used to store image features\n",
    "    features = dict()\n",
    "    \n",
    "    # Iterate through each images in given directory using for loop\n",
    "    for file_name in tqdm(os.listdir(directory)):\n",
    "        filename = f\"{directory}/{file_name}\"\n",
    "        # load each filename as image and resize the image to given target_size\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        # Convert image pixels to numpy array\n",
    "        image = img_to_array(image)\n",
    "        # Before presenting any data to CNN you may sometimes need to reshape your data\n",
    "        # We are reshaping the data without changing its content\n",
    "        image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
    "        # Preparing the image to fit to the VGG16 model\n",
    "        image = preprocess_input(image)\n",
    "        # Extracting the features from the image\n",
    "        # By setting verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch.\n",
    "        # verbose=0 will show you nothing (silent)\n",
    "        # verbose=1 will show you an animated progress bar like this: [===============================]\n",
    "        # verbose=2 will just mention the number of epoch like this: Epoch 1/10\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        # Getting the image id i.e. image12.jpg gets an image_id of image12\n",
    "        image_id = file_name.split('.')[0]\n",
    "        # Store the extracted feature to the empty 'features' dictionary created earlier\n",
    "        features[image_id] = feature\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8091 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8091/8091 [21:58<00:00,  6.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of all the extracted features is: 8091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "directory = './Datasets/Flicker8k_Dataset'\n",
    "features = get_image_features(directory)\n",
    "print(\"The length of all the extracted features is:\", len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the features to pickle file\n",
    "dump(features, open('./VGG16_features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocabulary of Image Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuction to transform descriptions into sets\n",
    "# The set object is the vacabulary of all the words\n",
    "def create_vocabulary(description):\n",
    "    all_desc = set()\n",
    "    # Creating a list of words in description text and each word is added to set created above\n",
    "    for key in description.keys():\n",
    "        [all_desc.update(desc.split()) for desc in description[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of descriptions: 8092\n",
      "The size of vocabulary is: 8763\n"
     ]
    }
   ],
   "source": [
    "# Calling the above created functions in a series\n",
    "filename = './Datasets/Flickr8k.token.txt'\n",
    "descriptions = read_image_descriptions(filename)\n",
    "print(f\"Total number of descriptions: {len(descriptions)}\")\n",
    "clean_description_text(descriptions)\n",
    "vocabulary = create_vocabulary(descriptions)\n",
    "print(f\"The size of vocabulary is: {len(vocabulary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Descriptions of Each Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a predefined list of image identifiers\n",
    "# This function first creates the list of image identifiers and then convert them to set object\n",
    "def get_identifiers_set(filename):\n",
    "    # Read the text from given file\n",
    "    # Open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # Read all the text from the file\n",
    "    text = file.read()\n",
    "    # After reading the file completely close the file\n",
    "    file.close()\n",
    "    \n",
    "    identifiers = list()\n",
    "    # Iterate though each line using for loop\n",
    "    for line in text.split('\\n'):\n",
    "        # Skip empty lines in case they exist\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # Grab the image identifier from each line and then add it to a list object created above\n",
    "        image_id = line.split('.')[0]\n",
    "        identifiers.append(image_id)\n",
    "    return set(identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion to load pre-processed cleaned descriptions\n",
    "# We will read descriptions from 'VGG16_descriptions.txt' file created earlier\n",
    "def get_clean_descriptions(filename, identifiers):\n",
    "    # Read the text from given file\n",
    "    # Open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # Read all the text from the file\n",
    "    text = file.read()\n",
    "    # After reading the file completely close the file\n",
    "    file.close()\n",
    "    \n",
    "    descriptions = dict()\n",
    "    # Iterate though each line using for loop\n",
    "    for line in text.split('\\n'):\n",
    "        # Tokenize the lines by splitting them according to whitespaces\n",
    "        tokens = line.split()\n",
    "        # Separate image identifiers from their descriptions\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # Skip all the images that do not belong to the above create set of image identifiers\n",
    "        if image_id in identifiers:\n",
    "            # Since single image contains 5 descriptions, we should not repeat image_id for all 5 descriptions separately\n",
    "            # If image_id is inserted for first description, other 4 descriptions should be added to the same key\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # Wrap descriptions in tokens\n",
    "            # starttoken and endtoken are tokens to signal the start and end of the caption\n",
    "            # We need these tokens because the captions are generated one word at a time\n",
    "            # These tokens are added to descriptions as they are loaded\n",
    "            # Later we need to encode these descriptions in our walk through\n",
    "            # We need to add these texts before encoding the text so that these tokens also get encoded\n",
    "            desc = \"starttoken \" + ' '.join(image_desc) + \" endtoken\"\n",
    "            descriptions[image_id].append(desc)\n",
    "            \n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the image features from the given dataset\n",
    "# filename argument is the pickle file (VGG16_features.pkl) that we created earlier\n",
    "# dataset argument is the training dataset\n",
    "def get_image_features(filename, dataset):\n",
    "    # Load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    # Load features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train identifiers is: 6000\n",
      "The length of train descriptions is: 6000\n",
      "The length of train features is: 6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['starttoken child in pink dress is climbing up set of stairs in an entry way endtoken',\n",
       " 'starttoken girl going into wooden building endtoken',\n",
       " 'starttoken little girl climbing into wooden playhouse endtoken',\n",
       " 'starttoken little girl climbing the stairs to her playhouse endtoken',\n",
       " 'starttoken little girl in pink dress going into wooden cabin endtoken']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load training dataset (6000 out of 2000) as present in Flickr_8k.trainImages.txt\n",
    "filename = './Datasets/Flickr_8k.trainImages.txt'\n",
    "# Get identifiers of all the training images\n",
    "train_id = get_identifiers_set(filename)\n",
    "print(f\"The length of train identifiers is: {len(train_id)}\")\n",
    "# Get cleaned/pre-processed descriptions from the file saved earlier for the training images\n",
    "train_desc = get_clean_descriptions('./VGG16_descriptions.txt', train_id)\n",
    "print(f\"The length of train descriptions is: {len(train_desc)}\")\n",
    "# Get features of all the training images from the pickle file saved earlier\n",
    "train_features = get_image_features('./VGG16_features.pkl', train_id)\n",
    "print(f\"The length of train features is: {len(train_features)}\")\n",
    "# Print list of training descriptions for given image_id\n",
    "# train_desc is the dictionary of image_id that consists of list of descriptions for that image_id\n",
    "train_desc['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No machine learning model can operate with text based data as input. And so, here we will convert our text data to numerical data so that our model can understand this data. Inorder to encode the data we create the mapping of the words to numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert dictionary of cleaned descriptions to list of cleaned descriptions\n",
    "def to_list(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(one_of_five_desc) for one_of_five_desc in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion to create tokens out of descriptions\n",
    "# i.e. Fucntion to tokenize the descriptions corpus\n",
    "def create_tokenizer(descriptions):\n",
    "    desc_list = to_list(descriptions)\n",
    "    # Keras has Tokenizer class that can learn the mapping of words to numerical values from loaded descriptions\n",
    "    tokenizer = Tokenizer()\n",
    "    # fit_on_texts() method updates internal vocabulary based on a list of texts\n",
    "    # This method creates the vocabulary index based on word frequency\n",
    "    # So if you give it something like, \"The cat sat on the mat.\"\n",
    "    # It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2\n",
    "    # So lower integer means more frequent word\n",
    "    # It is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding\n",
    "    tokenizer.fit_on_texts(desc_list)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is: 7579\n"
     ]
    }
   ],
   "source": [
    "# Get the size of vocabulary\n",
    "tokenizer = create_tokenizer(train_desc)\n",
    "# Adding 1 to the length because indexing starts from zero\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"The size of the vocabulary is: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding/Mapping Descriptions to Numerical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to calculate the length of the description having most words\n",
    "def max_lengths(descriptions):\n",
    "    desc_list = to_list(descriptions)\n",
    "    return max(len(desc.split()) for desc in desc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are ready to encode the text\n",
    "# Funtion to create a sequences of images, descriptions and outputs (next_words)\n",
    "def create_encoded_sequence(tokenizer, max_length, desc_list, image_feat):\n",
    "    # Instancing list objects to store image features, image descriptions and predicted next words\n",
    "    # Input text is encoded into numerical values\n",
    "    # So output text i.e predicted next word will also be one-hot encoded values\n",
    "    image_features, image_desc, next_words = list(), list(), list()\n",
    "    \n",
    "    # We'll iterate thorough each of the five descriptions of the given image using for loop\n",
    "    for desc in desc_list:\n",
    "        # texts_to_sequences() transforms each text in texts to a sequence of integers i.e. encode the sequence\n",
    "        # So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary.\n",
    "        # Only top (num_words-1) most frequent words will be taken into account. Only words known by the tokenizer will be taken into account.\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        \n",
    "        # Using for loop to split one sequence to multiple x,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # Split into input and output pairs\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad_sequences() is used to ensure that all sequences in a list have the same length.\n",
    "            # By default this is done by padding 0 in the beginning of each sequence until each sequence has the same length as the longest sequence.\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # to_categorical() method that can be used to one-hot encode integer data.\n",
    "            # If the integer data represents all the possible values of the classes, then the to_categorical() method can be used directly\n",
    "            # Otherwise, the number of classes can be passed to the method as the num_classes parameter.\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            image_features.append(image_feat)\n",
    "            image_desc.append(in_seq)\n",
    "            next_words.append(out_seq)\n",
    "            \n",
    "    return np.array(image_features), np.array(image_desc), np.array(next_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging CNN based models with LSTM models together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have two different models: \n",
    "1. Sequence Processor (RNN/LSTM)\n",
    "2. Feature Extractor (CNN)\n",
    "\n",
    "Both Image Feature Extractor and Sequence Processor generate fixed length output vectors separately. These separate output vectors are passed though a common Decoder and get merged together. Here both the vectors are processed by a Dense layer to generate the image description output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Embedding, Dense\n",
    "from tensorflow.keras.layers import Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that houses the entire structure of our model\n",
    "def create_model(vocab_size, max_length):\n",
    "    # Sequence processor model\n",
    "    input1 = Input(shape=(max_length,))\n",
    "    seq1 = Embedding(vocab_size, 256, mask_zero=True)(input1)\n",
    "    seq2 = Dropout(0.5)(seq1)\n",
    "    seq3 = LSTM(256)(seq2)\n",
    "    \n",
    "    # Feature extractor model\n",
    "    input2 = Input(shape=(4096,))\n",
    "    feat1 = Dropout(0.5)(input2)\n",
    "    feat2 = Dense(256, activation='relu')(feat1)\n",
    "    \n",
    "    # Decoder model\n",
    "    decoder1 = Add()([feat2, seq3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    output = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    \n",
    "    # Tie all the image features and word sequence together using keras Model class\n",
    "    model = Model(inputs=[input2, input1], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load one image worth of data per batch\n",
    "def data_generator(descriptions, image_feats, tokenizer, max_length):\n",
    "    # while loop is used to go over each image\n",
    "    while 1:\n",
    "        # for loop is used to extract features for given image\n",
    "        for key, desc_list in descriptions.items():\n",
    "            # Retrive the image features\n",
    "            image_feat = image_feats[key][0]\n",
    "            # Create sequence for single image (not entire data) for given batch\n",
    "            image_features, desc, next_words = create_encoded_sequence(tokenizer, max_length, desc_list, image_feat)\n",
    "            # yield is used to return from a function without destroying the states of its local variable\n",
    "            # When the function is called, the execution starts from the last yield statement.\n",
    "            # Any function that contains a yield keyword is termed as generator\n",
    "            yield ([image_features, desc], next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train identifiers is: 6000\n",
      "The length of train descriptions is: 6000\n",
      "The length of train features is: 6000\n",
      "The size of the vocabulary is: 7579\n",
      "The maximum length of train descriptions is: 34\n"
     ]
    }
   ],
   "source": [
    "# Load training dataset (6000 out of 2000) as present in Flickr_8k.trainImages.txt\n",
    "filename = './Datasets/Flickr_8k.trainImages.txt'\n",
    "# Get identifiers of all the training images\n",
    "train_id = get_identifiers_set(filename)\n",
    "print(f\"The length of train identifiers is: {len(train_id)}\")\n",
    "# Get cleaned/pre-processed descriptions from the file saved earlier for the training images\n",
    "train_desc = get_clean_descriptions('./VGG16_descriptions.txt', train_id)\n",
    "print(f\"The length of train descriptions is: {len(train_desc)}\")\n",
    "# Get features of all the training images from the pickle file saved earlier\n",
    "train_features = get_image_features('./VGG16_features.pkl', train_id)\n",
    "print(f\"The length of train features is: {len(train_features)}\")\n",
    "# Prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_desc)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"The size of the vocabulary is: {vocab_size}\")\n",
    "# Determine the maximum sequence length\n",
    "max_length = max_lengths(train_desc)\n",
    "print(f\"The maximum length of train descriptions is: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 34)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 34, 256)      1940224     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 34, 256)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          1048832     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          525312      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7579)         1947803     dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From <ipython-input-29-af9a9eef3d85>:10: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "6000/6000 [==============================] - 591s 98ms/step - loss: 4.6812\n",
      "6000/6000 [==============================] - 585s 98ms/step - loss: 3.9127\n",
      "6000/6000 [==============================] - 586s 98ms/step - loss: 3.6774\n",
      "6000/6000 [==============================] - 584s 97ms/step - loss: 3.5173\n",
      "6000/6000 [==============================] - 582s 97ms/step - loss: 3.4164\n",
      "6000/6000 [==============================] - 581s 97ms/step - loss: 3.3476\n",
      "6000/6000 [==============================] - 581s 97ms/step - loss: 3.2892\n",
      "6000/6000 [==============================] - 575s 96ms/step - loss: 3.2424\n",
      "6000/6000 [==============================] - 572s 95ms/step - loss: 3.2106\n",
      "6000/6000 [==============================] - 570s 95ms/step - loss: 3.1868\n"
     ]
    }
   ],
   "source": [
    "# Here we will train the model and save our model after each epoch as .h5 filename begining with model_\n",
    "model = create_model(vocab_size, max_length)\n",
    "epochs = 10\n",
    "# Steps is the size of train descriptions\n",
    "steps = len(train_desc)\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Create data generator\n",
    "    generator = data_generator(train_desc, train_features, tokenizer, max_length)\n",
    "    model.fit_generator(generator=generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save(f\"./VGG16_Models/model_{i}.h5\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation using BLEU Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Natural Language Processing, we may arise with multiple senarios where there may be multiple correct outputs. In such case, Accuracy Score is not a great metics to use and in those cases BLEU comes into paly.\n",
    "\n",
    "BLEU score stands for Bilingual Evaluation Understudy Score. BLEU, is a score for comparing a candidate translation of text to one or more reference translations. In simple language, BLEU Score is used to check how close the generated text is with respect to the expected text. Although developed for translation, it can be used to evaluate text generated for a suite of natural language processing tasks.\n",
    "\n",
    "The value of BLEU Score ranges from 0 to 1. Higher the BLEU Score, better will be the predicted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map a word using its corresponding word id\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        # Check if we have a match for given word at corresponding index\n",
    "        # If match found return the word else return None\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to generate a description for an image\n",
    "def generate_desc(model, tokenizer, image_feat, max_length):\n",
    "    # Seed/start the generation process\n",
    "    in_text = 'starttoken'\n",
    "    \n",
    "    # Iterate over the entire length of the sequence\n",
    "    # Here we will generate one word at a time by calling model recursively until 'END' string is detected\n",
    "    for i in range(max_length):\n",
    "        # Intiger encoded input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # Pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # Predict next word\n",
    "        yhat = model.predict([image_feat, sequence], verbose=0)\n",
    "        # Concert probability to integer\n",
    "        yhat = np.argmax(yhat)\n",
    "        # Map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # Stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # Add the generated next word to the original sequence\n",
    "        in_text += \" \" + word\n",
    "        if word == 'endtoken':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the skill of the model\n",
    "# i.e. determine how good the model is\n",
    "def evaluate_model(model, descriptions, image_feat, tokenizer, max_length):\n",
    "    # Instancing two lists one to store actual description and next to store predicted descriptions\n",
    "    reference, candidate = list(), list()\n",
    "    \n",
    "    # Iterate for the entire set of images\n",
    "    for key, desc_list in tqdm(descriptions.items()):\n",
    "        # Generate descriptions\n",
    "        yhat = generate_desc(model, tokenizer, image_feat[key], max_length)\n",
    "        # Storing actual/reference and predicted/candidate descriptions\n",
    "        references = [desc.split() for desc in desc_list]\n",
    "        reference.append(references)\n",
    "        candidate.append(yhat.split())\n",
    "    \n",
    "    print(f\"Cumulative 1-gram: {corpus_bleu(reference, candidate, weights=(1,0,0,0))}\")\n",
    "    print(f\"Cumulative 2-gram: {corpus_bleu(reference, candidate, weights=(0.5,0.5,0,0))}\")\n",
    "    print(f\"Cumulative 3-gram: {corpus_bleu(reference, candidate, weights=(0.33,0.33,0.33,0))}\")\n",
    "    print(f\"Cumulative 4-gram: {corpus_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train identifiers is: 6000\n",
      "The length of train descriptions is: 6000\n",
      "The size of the vocabulary is: 7579\n",
      "The maximum length of train descriptions is: 34\n"
     ]
    }
   ],
   "source": [
    "# Load training dataset (6000 out of 2000) as present in Flickr_8k.trainImages.txt\n",
    "filename_train = './Datasets/Flickr_8k.trainImages.txt'\n",
    "# Get identifiers of all the training images\n",
    "train_id = get_identifiers_set(filename_train)\n",
    "print(f\"The length of train identifiers is: {len(train_id)}\")\n",
    "# Get cleaned/pre-processed descriptions from the file saved earlier for the training images\n",
    "train_desc = get_clean_descriptions('./VGG16_descriptions.txt', train_id)\n",
    "print(f\"The length of train descriptions is: {len(train_desc)}\")\n",
    "\n",
    "# Prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_desc)\n",
    "# Save all the features to pickle file\n",
    "dump(tokenizer, open('./VGG16_tokenizer.pkl', 'wb'))\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"The size of the vocabulary is: {vocab_size}\")\n",
    "# Determine the maximum sequence length\n",
    "max_length = max_lengths(train_desc)\n",
    "print(f\"The maximum length of train descriptions is: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of test identifiers is: 1000\n",
      "The length of test descriptions is: 1000\n",
      "The length of test features is: 1000\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset (2000 out of 2000) as present in Flickr_8k.trainImages.txt\n",
    "filename_test = './Datasets/Flickr_8k.testImages.txt'\n",
    "# Get identifiers of all the test images\n",
    "test_id = get_identifiers_set(filename_test)\n",
    "print(f\"The length of test identifiers is: {len(test_id)}\")\n",
    "# Get cleaned/pre-processed descriptions from the file saved earlier for the test images\n",
    "test_desc = get_clean_descriptions('./VGG16_descriptions.txt', test_id)\n",
    "print(f\"The length of test descriptions is: {len(test_desc)}\")\n",
    "\n",
    "# Load test image features\n",
    "test_features = get_image_features('./VGG16_features.pkl', test_id)\n",
    "print(f\"The length of test features is: {len(test_desc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:56<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 0.5131110694886682\n",
      "Cumulative 2-gram: 0.2716642562928756\n",
      "Cumulative 3-gram: 0.15624772406037768\n",
      "Cumulative 4-gram: 0.0829891123781619\n"
     ]
    }
   ],
   "source": [
    "# Load the model which has minimum loss\n",
    "# Here I am working with model_9.h5\n",
    "filename = './VGG16_Models/model_9.h5'\n",
    "model = load_model(filename)\n",
    "# Evaluate model\n",
    "evaluate_model(model, test_desc, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Image Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuction to extract features from each image to the directory\n",
    "def extract_test_features(filename):\n",
    "    # Create an instance of the VGG16 model\n",
    "    model = VGG16()\n",
    "    # Restructuring our VGG16 model by removing/Popping off the last layer of the model\n",
    "    # The last layer is used to classify the images. Since we are not classifying images here, we're removing the last layer\n",
    "    model.layers.pop()\n",
    "    # Keras model represents the actual neural network model.\n",
    "    # Keras provides a two mode to create the model, simple and easy to use Sequential API as well as more flexible and advanced Functional API.\n",
    "    # A ANN model can be created by simply calling Sequential() API\n",
    "    # Sequential API is used to create models layer-by-layer\n",
    "    # Functional model, you can define multiple input or output that share layers.\n",
    "    # First, we create an instance for model using Model class and connect to the layers to access input and output to the model\n",
    "    # model.inputs is the input fed to the model and model.layers[-1].output is the output of the last(-1) layer of the model\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\n",
    "    # load each filename as image and resize the image to given target_size\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    # Convert image pixels to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # Before presenting any data to CNN you may sometimes need to reshape your data\n",
    "    # We are reshaping the data without changing its content\n",
    "    image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
    "    # Preparing the image to fit to the VGG16 model\n",
    "    image = preprocess_input(image)\n",
    "    # Extracting the features from the image\n",
    "    # By setting verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch.\n",
    "    # verbose=0 will show you nothing (silent)\n",
    "    # verbose=1 will show you an animated progress bar like this: [===============================]\n",
    "    # verbose=2 will just mention the number of epoch like this: Epoch 1/10\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starttoken man in red shirt is riding bicycle down the street endtoken\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load(open('./VGG16_tokenizer.pkl', 'rb'))\n",
    "# Pre-define the maximum sequence length (from taining)\n",
    "max_length = 34\n",
    "# Load the model with minimum error\n",
    "model = load_model('./VGG16_Models/model_9.h5')\n",
    "# Load the image of which you need to generate description\n",
    "test_image = extract_test_features('./cycle.jpg')\n",
    "# Generate description of the image\n",
    "description = generate_desc(model, tokenizer, test_image, max_length)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man in red shirt is riding bicycle down the street\n"
     ]
    }
   ],
   "source": [
    "# Remove start and end tokens\n",
    "query = description\n",
    "stopwords = ['starttoken', 'endtoken']\n",
    "query_words = query.split()\n",
    "result = [word for word in query_words if word not in stopwords]\n",
    "result = ' '.join(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
