{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Image Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# preprocess_input() is used to preprocess any given image to extract features of that image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "# load_img() is used to load image from file as a pil image\n",
    "# img_to_array() is used to convert pil image instance to a numpy array so that our model can understand/interpret the image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "# Model() can be instanciated to include the necessary layers given some input arrays or tensors to output arrays or tensors\n",
    "from tensorflow.keras.models import Model\n",
    "# Pickle is used to serialize and deserialize the python object structure so that any object on python can be pickled and saved to the disk\n",
    "# So pickle.load() fuction is used to load object data from the pickle file\n",
    "from pickle import load\n",
    "# load_model() fuction is used to load saved models from .h5 file\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map a word using its corresponding word id\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        # Check if we have a match for given word at corresponding index\n",
    "        # If match found return the word else return None\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to generate a description for an image\n",
    "def generate_desc(model, tokenizer, image_feat, max_length):\n",
    "    # Seed/start the generation process\n",
    "    in_text = 'starttoken'\n",
    "    \n",
    "    # Iterate over the entire length of the sequence\n",
    "    # Here we will generate one word at a time by calling model recursively until 'END' string is detected\n",
    "    for i in range(max_length):\n",
    "        # Intiger encoded input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # Pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # Predict next word\n",
    "        yhat = model.predict([image_feat, sequence], verbose=0)\n",
    "        # Concert probability to integer\n",
    "        yhat = np.argmax(yhat)\n",
    "        # Map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # Stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # Add the generated next word to the original sequence\n",
    "        in_text += \" \" + word\n",
    "        if word == 'endtoken':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuction to extract features from each image to the directory\n",
    "def extract_test_features(filename):\n",
    "    # Create an instance of the VGG16 model\n",
    "    model = VGG16()\n",
    "    # Restructuring our VGG16 model by removing/Popping off the last layer of the model\n",
    "    # The last layer is used to classify the images. Since we are not classifying images here, we're removing the last layer\n",
    "    model.layers.pop()\n",
    "    # Keras model represents the actual neural network model.\n",
    "    # Keras provides a two mode to create the model, simple and easy to use Sequential API as well as more flexible and advanced Functional API.\n",
    "    # A ANN model can be created by simply calling Sequential() API\n",
    "    # Sequential API is used to create models layer-by-layer\n",
    "    # Functional model, you can define multiple input or output that share layers.\n",
    "    # First, we create an instance for model using Model class and connect to the layers to access input and output to the model\n",
    "    # model.inputs is the input fed to the model and model.layers[-1].output is the output of the last(-1) layer of the model\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\n",
    "    # load each filename as image and resize the image to given target_size\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    # Convert image pixels to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # Before presenting any data to CNN you may sometimes need to reshape your data\n",
    "    # We are reshaping the data without changing its content\n",
    "    image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
    "    # Preparing the image to fit to the VGG16 model\n",
    "    image = preprocess_input(image)\n",
    "    # Extracting the features from the image\n",
    "    # By setting verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch.\n",
    "    # verbose=0 will show you nothing (silent)\n",
    "    # verbose=1 will show you an animated progress bar like this: [===============================]\n",
    "    # verbose=2 will just mention the number of epoch like this: Epoch 1/10\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starttoken little boy in pink shirt is playing in the air endtoken\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load(open('./VGG16_tokenizer.pkl', 'rb'))\n",
    "# Pre-define the maximum sequence length (from taining)\n",
    "max_length = 34\n",
    "# Load the model with minimum error\n",
    "model = load_model('./VGG16_Models/model_9.h5')\n",
    "# Load the image of which you need to generate description\n",
    "test_image = extract_test_features('./test.jpg')\n",
    "# Generate description of the image\n",
    "description = generate_desc(model, tokenizer, test_image, max_length)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little boy in pink shirt is playing in the air\n"
     ]
    }
   ],
   "source": [
    "# Remove start and end tokens\n",
    "query = description\n",
    "stopwords = ['starttoken', 'endtoken']\n",
    "query_words = query.split()\n",
    "result = [word for word in query_words if word not in stopwords]\n",
    "result = ' '.join(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Generated Text to Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = pyttsx3.init('espeak')\n",
    "\n",
    "rate = engine.getProperty('rate')\n",
    "engine.setProperty('rate', 200)\n",
    "\n",
    "volume = engine.getProperty('volume')\n",
    "engine.setProperty('volume', 1.0)\n",
    "\n",
    "voices = engine.getProperty('voices')\n",
    "engine.setProperty('voice', voices[1].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(sound):\n",
    "    engine.say(sound)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    speak(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (IntelÂ® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
